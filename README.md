# HPC-Twitter-GeoProcessing
The task is to take a large JSON file of data, 11 GB, and implement a simple and parallelized program on Melbourne Universities HPC known as Spartan.  In order to process this amount of data fast, it is not possible to use one single computers processing power as the job is too large. Therefore, the purpose of this assignment is to understand how to implement a system that can take advantage of the processing power of a clustered system of computers, but complete the job and provide results as if the job had been completed by a single computer.  This application will take geocoded Twitter data and identify Twitter usage around Melbourne and the most frequently occurring hashtags that are being sent from those areas. Further to this, the difference in performance between 1 node with 1 core, 2 nodes with 8 cores and 2 nodes with eight cores will be analyzed. 
